require 'torch'
require 'cutorch'
require 'cunn'
require 'optim'
require 'xlua'

require 'hdf5'

require 'StackShift'

-- Implementing: https://gist.github.com/shelhamer/80667189b218ad570e82#file-readme-md

-- Network size configurations
-- TODO: make these automatic
-- The number of output classes
n_classes = 13

-- The size of the images being processed
height = 480
width = 640

-- Enumeration of the classes
classes = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '12'}

-- Set the default tensor type for cpu use
torch.setdefaulttensortype("torch.FloatTensor")

-- Training function
function train()

	-- For counting the data items
	local count = 0

	-- For tracking the loss of the network
	current_loss = 0

	-- Epoch
	--epoch = epoch or 1

	-- Timer
	--local time = sys.clock()

	-- Train for one epoch
	--print("Training epoch: " .. epoch)

	-- Get the shape of the data set
	data_shape = data_set:dataspaceSize()
	n_images = data_shape[1]
	height = data_shape[2]
	width = data_shape[3]

	-- Go through each image one at a time
	for i = 1, n_images - 1 do

		io.write("\rWorking on image " .. i)

		collectgarbage()

		-- Load the data item
		local data_item = data_set:partial({i,i},{1,height},{1,width}):cuda()
		--local data_item = data_set:partial({i,i},{1,height},{1,width})

		-- load the vector label
		--local label_item = label_set:partial({i,i}, {1,height}, {1,width}, {1,n_classes}):cuda()
		local label_item = label_set:partial({i,i}, {1,height}, {1,width}):squeeze():cuda()
		--local label_item = label_item:view(label_item:nElement())
		--label_item = label_set:partial({i,i}, {1,height}, {1,width}):squeeze()
		label_item = label_item:view(label_item:nElement()) + 1

		-- Closure for feval to get gradients
		local feval = function(x_new)

			--collectgarbage()

			-- Reset the data
			if x~= x_new then x:copy(x_new) end
			dl_dx:zero()

			-- Run the forward input
			output = model:forward(data_item)

			--print ("d")
			--print (output:size())
			--print ("l")
			--print (label_item:size())

			-- Gradient descent for the image
			local loss = criterion:forward(model.output, label_item)
			model:backward(data_item, criterion:backward(model.output, label_item))

			return loss, dl_dx
		end

		--collectgarbage()

		_, fs = optim.sgd(feval, x, sgd_params)

		io.write(" loss " .. fs[1])
		io.flush()

		count = count + 1

		current_loss = current_loss + fs[1]

		-- Every 5000 images, do a temp save and test the model
		if i % 5000 == 0 then

			test()
			model:training()

			torch.save('temp.dat', model)

		end

	end

	io.write("\n")

	return current_loss / count

end

-- Test function
function test()

	-- The confusion matrix
	local confusion = optim.ConfusionMatrix(13)

	-- Total number of images being tested
	local test_im_total = test_set:dataspaceSize()[1]

	-- Set to evaluate to get production mode
	model:evaluate()

	-- Go through each image in the test set
	--for i = 1, test_im_total do
	for i = 1, 1 do

		-- Progress bar
		--xlua.progress(i, test_im_total)

		-- Get an image
		local data_item = test_set:partial({i,i},{1,height},{1,width}):cuda()

		-- load the vector label
		local label_item = test_label_set:partial({i,i}, {1,height}, {1,width}):squeeze():cuda()
		label_item = label_item:view(label_item:nElement()) + 1

		-- Get the forward prediction of the log probabilities
		local prediction_log_probs = model:forward(data_item)

		-- Go through each pixel
		-- TODO: More efficient way?
		for j = 1, prediction_log_probs:size()[1] do

			xlua.progress(j, prediction_log_probs:size()[1])

			-- Get the label that this prediction corresponds to
			local ignore, prediction = torch.max(prediction_log_probs[j], 1)

			-- Update the confusion matrix
			--print("con start")
			--print(prediction[1])
			--print(label_item[j])
			confusion:add(prediction[1], label_item[j])
			--print("con add")

		end

	end

	-- Show the confusion matrix
	print("Confusion_matrix")
	print(confusion)

end

-- Where to load data from
--load_set = "/media/ebcf6e76-2430-41c9-917a-d331f6258c57/occulsion_data/Easy_set_01.hdf5"
--load_set = "/home/master/will-it-blend/generated_data/vect_test.hdf5"
--load_set = "/media/6a2ce75c-12d0-4cf5-beaa-875c0cd8e5d8/occulsion_data/Easy_set_01_no_vect_test.hdf5"

load_set = "/media/6a2ce75c-12d0-4cf5-beaa-875c0cd8e5d8/Easy_set_01_threshold.hdf5"

print("Loading data from:")
print(load_set)

-- Get the handle for the whole data set
whole_set = hdf5.open(load_set, 'r')

-- Get the handles for the data and label sets
data_set = whole_set:read('data')
label_set = whole_set:read('label')

print("Data shape:")
print(data_set:dataspaceSize())

print("Label shape:")
print(label_set:dataspaceSize())

-- Test data
--test_name = "/media/6a2ce75c-12d0-4cf5-beaa-875c0cd8e5d8/Easy_set_01_test.hdf5"
test_name = "/media/6a2ce75c-12d0-4cf5-beaa-875c0cd8e5d8/Easy_set_01_threshold.hdf5"

print("Loading test set from:")
print(test_name)

-- Open the test set
whole_test_set = hdf5.open(test_name, 'r')

-- Get the handles for the test data and labels
test_set = whole_test_set:read('data')
test_label_set = whole_test_set:read('label')

-- Set the GPU to use
cutorch.setDevice(1)

print("Creating the model")
-- The last main line convolution and pooling
-- Starts from pool4
final_main = nn.Sequential()

-- Tracks the amount of sampling
sampling = 1

-- The kernel size to use for all standard conv layers
k_size = 3

-- The padding needed on standard conv layers to keep shape right
pad = torch.floor((k_size - 1)/2)

-- conv1
--conv1_planes = 64
conv1_planes = 32

final_main:add(nn.SpatialConvolution(1, conv1_planes, k_size, k_size, 1, 1, pad, pad)) -- conv1_1
final_main:add(nn.ReLU())
final_main:add(nn.SpatialConvolution(conv1_planes, conv1_planes, k_size, k_size, 1, 1, pad, pad)) -- conv1_2
final_main:add(nn.ReLU())

-- pool1
sampling = 2 * sampling
final_main:add(nn.SpatialMaxPooling(2, 2, 2, 2))

-- conv2
--conv2_planes = 128
conv2_planes = 64

final_main:add(nn.SpatialConvolution(conv1_planes, conv2_planes, k_size, k_size, 1, 1, pad, pad)) -- conv2_1
final_main:add(nn.ReLU())
final_main:add(nn.SpatialConvolution(conv2_planes, conv2_planes, k_size, k_size, 1, 1, pad, pad)) -- conv2_2
final_main:add(nn.ReLU())

-- pool2
sampling = 2 * sampling
final_main:add(nn.SpatialMaxPooling(2, 2, 2, 2))

-- conv3
--conv3_planes = 256
conv3_planes = 128

final_main:add(nn.SpatialConvolution(conv2_planes, conv3_planes, k_size, k_size, 1, 1, pad, pad)) -- conv3_1
final_main:add(nn.ReLU())
final_main:add(nn.SpatialConvolution(conv3_planes, conv3_planes, k_size, k_size, 1, 1, pad, pad)) -- conv3_2
final_main:add(nn.ReLU())
final_main:add(nn.SpatialConvolution(conv3_planes, conv3_planes, k_size, k_size, 1, 1, pad, pad)) -- conv3_3
final_main:add(nn.ReLU())

-- pool3
sampling = 2 * sampling
final_main:add(nn.SpatialMaxPooling(2, 2, 2, 2))

-- conv4
--conv4_planes = 512
conv4_planes = 256

final_main:add(nn.SpatialConvolution(conv3_planes, conv4_planes, k_size, k_size, 1, 1, pad, pad)) -- conv4_1
final_main:add(nn.ReLU())
final_main:add(nn.SpatialConvolution(conv4_planes, conv4_planes, k_size, k_size, 1, 1, pad, pad)) -- conv4_2
final_main:add(nn.ReLU())
final_main:add(nn.SpatialConvolution(conv4_planes, conv4_planes, k_size, k_size, 1, 1, pad, pad)) -- conv4_3
final_main:add(nn.ReLU())

-- pool4
sampling = 2 * sampling
final_main:add(nn.SpatialMaxPooling(2, 2, 2, 2))

-- conv5
--conv5_planes = 512
conv5_planes = 256

final_main:add(nn.SpatialFullConvolution(conv4_planes, conv5_planes, k_size, k_size, 1, 1, pad, pad)) -- conv5_1
final_main:add(nn.ReLU())
final_main:add(nn.SpatialFullConvolution(conv5_planes, conv5_planes, k_size, k_size, 1, 1, pad, pad)) -- conv5_2
final_main:add(nn.ReLU())
final_main:add(nn.SpatialFullConvolution(conv5_planes, conv5_planes, k_size, k_size, 1, 1, pad, pad)) -- conv5_3

-- pool5
sampling = 2 * sampling
final_main:add(nn.SpatialMaxPooling(2, 2, 2, 2))

-- conv6-7
-- aka fc6-7
--fc6_7_planes = 4096 --Amount in paper, but uses too much memory
--fc6_7_planes = 3072
--fc6_7_planes = 1024 -- Works on GTX 960
fc6_7_planes = 1024

final_main:add(nn.SpatialFullConvolution(conv5_planes, fc6_7_planes, 7, 7, 1, 1, 3, 3)) -- fc6
final_main:add(nn.ReLU())
final_main:add(nn.Dropout(.5))
final_main:add(nn.SpatialFullConvolution(fc6_7_planes, fc6_7_planes, 1, 1)) -- fc7
final_main:add(nn.ReLU())
final_main:add(nn.Dropout(.5))

-- output layer
final_main:add(nn.SpatialFullConvolution(fc6_7_planes, n_classes, 1, 1))
--final_main:add(nn.SpatialFullConvolution(conv5_planes, n_classes, 1, 1))

-- Upscale all of the previous downsamplings
--final_main:add(nn.SpatialUpSamplingNearest(sampling))
final_main:add(nn.SpatialFullConvolution(n_classes, n_classes, 2*sampling, 2*sampling, sampling, sampling, sampling/2, sampling/2))

-- Make all values 0 - 1
--final_main:add(nn.Sigmoid())

-- Reorder and reshape for the MMC criterion
--final_main:add(nn.StackShift())

final_main:add(nn.Reshape(height * width, n_classes))

-- For using ClassNLL
-- Needs LogSoftMax layer
-- Needs to be reshaped to width * height, n_classes
-- WARNING: the network is only suitable for running one image at a time because of this
final_main:add(nn.LogSoftMax())

-- Put the model together
-- Nothing to do for this right now
model = final_main

-- Move to GPU
model:cuda()

print("Constructed model:")
print(model)
--os.execute("sleep " .. 10)

-- Criterion, use the classic MSE for now
--criterion = nn.MSECriterion():cuda()
--criterion = nn.CrossEntropyCriterion():cuda()
--criterion = nn.MultiMarginCriterion():cuda()
criterion = nn.ClassNLLCriterion():cuda()

-- Parameters for SGD
--[[
sgd_params = {
	learningRate = 1e-2,
	learningRateDecay = 1e-4,
	weightDecay = 1e-3,
	momentum = 1e-4
}
--]]
sgd_params = {
	learningRate = .5,
	learningRateDecay = 1e-4,
	weightDecay = 1e-3,
	momentum = .9
}

-- TODO what is this exactly
x, dl_dx = model:getParameters()

-- TEMP test net forward with random data
function rand_forward()
	local rand_data = torch.randn(480 * 640):reshape(1, 480, 640)
	rand_data = rand_data:cuda()
	print ("Input shape:")
	print (rand_data:size())

	-- Time the propagation
	timer = torch.Timer()

	-- Run the image through
	local out = final_main:forward(rand_data)

	-- Get the end time
	end_time = timer:time().real

	print("Out shape:")
	print(out:size())

	print("Time taken for forward propagation of 1 image:")
	print(end_time)
	require 'os'
	os.execute("sleep " .. 10)
	rand_data = nil
	out = nil
	collectgarbage(); collectgarbage()
	print("gc")
	os.execute("sleep " .. 10)
end
--print("forward")
--rand_forward()
--collectgarbage()
--print("another")
--rand_forward()

-- Time the training
timer = torch.Timer()

-- Do a dry run test
--print("Doing a random test")
--test()

-- Run for the specified number of epochs
max_epochs = 10
for epoch = 1, max_epochs do

	-- Do the training
	this_loss = train()

	print("Loss at epoch " .. epoch .. ": " .. this_loss)

	-- Do a test
	test()

end

-- Run one training set
--one_loss = train()

-- Get the end time
end_time = timer:time().real

print("Training complete")
print("Average time taken for training an epoch: " .. end_time / max_epochs)
print("Total time: " .. end_time)

-- Save the completed model
torch.save('test_model.dat', model)

--print("Loss after one epoch:")
--print(one_loss)
--os.execute("sleep " .. 10)

-- Do a forward propagation to see the results

-- Close the hdf5
whole_set:close()
